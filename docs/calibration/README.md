# Phase 3: Calibration & Validation

This directory contains documentation for validating that cc-plugin-eval's LLM judges produce results that correlate with human expert judgment. The goal is to establish framework credibility by demonstrating statistically significant agreement between automated and human evaluation.

## Why Calibration Matters

The evaluation framework uses LLM judges to assess plugin quality across multiple dimensions (trigger accuracy, technical correctness, completeness). Before trusting these automated assessments, we need to verify they align with how human experts would evaluate the same plugins.

Anthropic's Bloom framework achieved a Spearman correlation of Ï = 0.86 with human judgment using 40 labeled transcripts. We target similar rigor: **Ï â‰¥ 0.80** across our evaluation dimensions.

## Documents in This Directory

| Document                                             | Purpose                                                                          |
| ---------------------------------------------------- | -------------------------------------------------------------------------------- |
| [corpus.md](corpus.md)                               | The 32-plugin calibration corpus spanning polished â†’ experimental quality levels |
| [human-evaluation.md](human-evaluation.md)           | Procedures for human labeling, blind evaluation protocol, CLI tool specification |
| [llm-judge-calibration.md](llm-judge-calibration.md) | Judge prompt versioning, sampling strategy, temperature settings                 |

## Calibration Workflow

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Phase 3 Calibration Process                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. CORPUS ASSEMBLY (corpus.md)
   â””â”€â”€ 32 plugins across 4 maturity levels
       â”œâ”€â”€ Polished (7)     â†’ Gold standard references
       â”œâ”€â”€ Functional (12)  â†’ Working with varying docs
       â”œâ”€â”€ Rough (7)        â†’ Known issues, discrimination testing
       â””â”€â”€ Experimental (6) â†’ Edge cases, unconventional patterns

2. SCENARIO GENERATION
   â””â”€â”€ Run cc-plugin-eval stages 1-2 on each corpus plugin
       â””â”€â”€ ~5-10 scenarios per plugin = 160-320 total scenarios

3. HUMAN LABELING (human-evaluation.md)
   â””â”€â”€ Blind evaluation of scenario subsets
       â”œâ”€â”€ Trigger accuracy (most objective, start here)
       â”œâ”€â”€ Technical correctness
       â””â”€â”€ Completeness
   â””â”€â”€ Multiple labelers for inter-rater reliability

4. LLM JUDGE EXECUTION (llm-judge-calibration.md)
   â””â”€â”€ Run stage 4 evaluation on same scenarios
       â”œâ”€â”€ Record judge verdicts and confidence
       â””â”€â”€ Track prompt versions for reproducibility

5. CORRELATION ANALYSIS
   â””â”€â”€ Calculate Spearman Ï between human and LLM judgments
       â”œâ”€â”€ Per-dimension correlation
       â”œâ”€â”€ Aggregate correlation
       â””â”€â”€ Confidence intervals via bootstrap
   â””â”€â”€ Target: Ï â‰¥ 0.80

6. ITERATION
   â””â”€â”€ If correlation insufficient:
       â”œâ”€â”€ Refine judge prompts
       â”œâ”€â”€ Adjust evaluation dimensions
       â””â”€â”€ Expand labeled dataset
```

## Current Status

| Milestone            | Status         | Notes                              |
| -------------------- | -------------- | ---------------------------------- |
| Corpus assembly      | âœ… Complete    | 32 plugins documented in corpus.md |
| Scenario generation  | ğŸ”² Not started | Pending corpus cloning             |
| Human labeling tool  | ğŸ”² Not started | CLI spec in human-evaluation.md    |
| Initial labeling     | ğŸ”² Not started | Start with trigger accuracy        |
| LLM judge baseline   | ğŸ”² Not started | Use current stage 4 prompts        |
| Correlation analysis | ğŸ”² Not started | â€”                                  |

## Key Decisions

**Starting with trigger accuracy**: Of the four evaluation dimensions (trigger accuracy, technical accuracy, code validity, completeness), trigger accuracy is the most objective and easiest to label. A scenario either triggered the expected component or it didn't. This makes it ideal for initial calibration work.

**Blind evaluation**: Human labelers evaluate scenarios without seeing LLM judge verdicts to prevent anchoring bias.

**Inter-rater reliability**: Where feasible, multiple labelers evaluate the same scenarios to measure agreement (Cohen's Îº or Krippendorff's Î±).

## References

- [Bloom Technical Report](https://alignment.anthropic.com/2025/bloom-auto-evals) â€” Anthropic's approach to LLM-as-judge validation
- [Bloom GitHub](https://github.com/safety-research/bloom) â€” Reference implementation
